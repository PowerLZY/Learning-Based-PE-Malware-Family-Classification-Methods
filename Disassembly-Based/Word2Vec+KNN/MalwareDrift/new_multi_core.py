#'parallel time: ', 55939.9327788353s

import sys
import os
import time

import gensim
import pickle


import numpy as np
import multiprocessing as mp


malware_train_docs = dict()
malware_test_dir_1 = 'fold_6_after'
malware_test_dir_2 = 'fold_5_test'
all_malware_dir = 'parsed_malware_slow_method_before'
fold = 'fold_6_after_5'

for root, dirs, filenames in os.walk(all_malware_dir):
        for malware in filenames:
                malware_train_docs[malware] = open(os.path.join(root, malware), 'rb').read().splitlines()


vocabulary = []
for malware in malware_train_docs:
        vocabulary.append(malware_train_docs[malware])


#alg = int(sys.argv[1])
#win = int(sys.argv[2])
alg = 1
win = 5
model = gensim.models.Word2Vec(vocabulary, workers=8, sg=alg, window=win, min_count=1)
print('vocabulary size  =  ' + str(len(model.wv.vocab)))
print('algorithm  =  ' + str(model.sg))
print('window size  =  ' + str(model.window))

print('calculating distances...\n')
start = time.time()

bad_malwares = []

# remove test files (test_files_2) from all files
all_filenames = os.listdir(all_malware_dir)
test_filenames = os.listdir(malware_test_dir_2)
train_filenames = []
for file in all_filenames:
    flag = 1
    for test in test_filenames:
        if file == test:
            flag = 0
            break
    if flag == 1:
        train_filenames.append(file)



file_names = os.listdir(malware_test_dir_1)



def single_run(distances, data):
    print("{} start.".format(os.getpid()))
    for test_malware in data:
        print(str(test_malware) + " processing...")
        if test_malware in bad_malwares:
                continue
        malware_test_doc = open(malware_test_dir_1 + '/' + test_malware, 'r').read().splitlines()
        local_distance = dict()

        train_malwares = train_filenames[:]

        for malware in train_malwares:
            if malware in bad_malwares:
                continue
            try:
                distance = model.wmdistance(malware_train_docs[malware], malware_test_doc)
                local_distance[malware] = distance
                # distances[test_malware][malware] = distance
            except UnicodeDecodeError as err:
                if err.object in malware_train_docs[malware]:
                    bad_malwares.append(malware)
                else:
                    bad_malwares.append(test_malware)
        distances[test_malware] = local_distance
        print(str(test_malware) + " done. Dictionary length: " + str(len(distances)))
    print("{} finished.".format(os.getpid()))


def one_by_one():   # single_core
    begin_time = time.time()
    file_names = os.listdir(malware_test_dir_1)
    # digits = [1, 2, 3, 4, 5, 6]
    single_run(file_names)
    end_time = time.time()
    print("one by one time: ", end_time-begin_time)



if __name__ == '__main__':
    start = time.time()
    m = mp.Manager()
    d = m.dict()  # 50 process used

    n = len(file_names)  # 1824
    procs = []  # 50 procs
#   n_cpu = mp.cpu_count()
    n_cpu = 75
    chunk_size = int(n / n_cpu)

    for i in range(0, n_cpu):
        min_i = chunk_size * i
        if i < n_cpu - 1:
            max_i = chunk_size * (i + 1)
        else:
            max_i = n
        digits = []
        for num in range(min_i, max_i):
            digits.append(file_names[num])
        print(digits)
        t = mp.Process(target=single_run, args=(d, digits))
        t.daemon = True
        procs.append(t)

    for proc in procs:
        proc.start()
    for proc in procs:
        proc.join()


 #   for i in range(len(procs)):
 #       print(i)
 #       locals()['tmp%d' % i] = q.get()
    print('begin to merge the dictionary')

    end = time.time()
    distances = {}
    for key in d.keys():
        distances[key] = d.get(key)

    ## write distances dictionary to file
    with open(fold + '_distances_win' + str(win) + '_sg' + str(alg) + '.pickle', 'wb') as handle:
        pickle.dump(distances, handle, protocol=pickle.HIGHEST_PROTOCOL)

    print("parallel time: %.8f" % (end-start))





