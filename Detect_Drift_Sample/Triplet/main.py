import os
os.environ['PYTHONHASHSEED'] = '0'
from numpy.random import seed
import random
random.seed(1)
seed(1)
from tensorflow import set_random_seed
set_random_seed(2)

import sys
import logging
import traceback

from timeit import default_timer as timer
from pprint import pformat
from collections import Counter
from keras.models import load_model
from keras import backend as K
from keras.utils import np_utils
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"  # so the IDs match nvidia-smi
os.environ["CUDA_VISIBLE_DEVICES"] = "0"



from Triplet.Get_emb_label import *



# TensorFlow wizardry
config = tf.ConfigProto()
# Don't pre-allocate memory; allocate as-needed
config.gpu_options.allow_growth = True
# Only allow a total of half the GPU memory to be allocated
config.gpu_options.per_process_gpu_memory_fraction = 0.5
# Create a session with the above options specified.
K.tensorflow_backend.set_session(tf.Session(config=config))


import argparse
from Triplet.utils import Params
from collections import OrderedDict


parser = argparse.ArgumentParser()
parser.add_argument('--model_dir', default='experiments/',
                    help="Experiment directory containing params.json")







def main():

    # Load the parameters from json file
    args = parser.parse_args()
    json_path = os.path.join(args.model_dir, 'params.json')
    assert os.path.isfile(json_path), "No json configuration file found at {}".format(json_path)
    params = Params(json_path)

    # --------------------------------------------------------- #
    # 1. Detect drifting samples in the testing set                  #
    # --------------------------------------------------------- #
    logging.info('Detect drifting samples in the testing set...')

    # ALL: contains the closest family for all the testing set, use this to compare with classifier's prediction
    ALL_DETECT_PATH = os.path.join('report', 'detect_results_all.csv')

    # SIMPLE: only contains drift samples flagged by the MAD # NOTE: this is just for quickly viewing the results
    SIMPLE_DETECT_PATH = os.path.join('report', 'detect_results_simple.csv')

    training_info_for_detect_path = os.path.join('report', 'training_info_for_detect.npz')


    train_emb, train_lab = get_train_emb_label()
    test_emb, test_lab = get_test_emb_label()



    s2 = timer()
    detect_drift_samples_myself(train_emb, train_lab, test_emb, test_lab,
                                margin=params.margin,
                                mad_threshold=3.5,
                                all_detect_path=ALL_DETECT_PATH,
                                simple_detect_path=SIMPLE_DETECT_PATH,
                                training_info_for_detect_path=training_info_for_detect_path
                                )

    e2 = timer()
    logging.debug(f'detect_odd_samples time: {(e2 - s2):.3f} seconds')
    logging.info('Detect drifting samples in the testing set finished')





    # --------------------------------------------------------- #
    # 2. Evaluate the detection performance                     #
    # --------------------------------------------------------- #

    DETECT_RESULT_ALL = 'report/detect_results_all.csv'

    SAVE_ORDERED_DIS_PATH = os.path.join('report', 'ordered_sample_by_min_dis.txt')

    # final result
    DIST_EFFORT_PR_VALUE_FIG_PATH = os.path.join('figure',
                                                 'inspection_effort_PreAndRecall_value.png')
    DIST_ONE_BY_ONE_CHECK_RESULT_PATH = os.path.join('report',
                                                     'one_by_one_check_PreAndRecall_value.csv')


    evaluate_newfamily_as_drift_by_distance_myself(  newfamily=7,
                                                     helper_file=DETECT_RESULT_ALL,
                                                     mad_threshold=3.5,
                                                     save_ordered_dis_path=SAVE_ORDERED_DIS_PATH,
                                                     dist_effort_pr_value_fig_path=DIST_EFFORT_PR_VALUE_FIG_PATH,
                                                     dist_one_by_one_check_result_path=DIST_ONE_BY_ONE_CHECK_RESULT_PATH)

    print('Evaluate the detection performance finished')








if __name__ == "__main__":
    start = timer()
    main()
    end = timer()
    logging.info(f'time elapsed: {end - start}')
