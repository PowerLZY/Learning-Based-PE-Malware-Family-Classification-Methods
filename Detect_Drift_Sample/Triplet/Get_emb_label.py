import os
import pathlib
import shutil

import numpy as np
import tensorflow as tf
import traceback


# from tensorflow.contrib.tensorboard.plugins import projector
from tensorboard.plugins import projector

import Triplet.malware_dataset as mnist_dataset
from Triplet.utils import Params
from Triplet.input_fn import test_input_fn,train_input_fn_detect
from Triplet.model_fn import model_fn

import matplotlib.pyplot as plt
import argparse
import logging
from collections import OrderedDict


parser = argparse.ArgumentParser()
parser.add_argument('--model_dir', default='experiments/',
                    help="Experiment directory containing params.json")
parser.add_argument('--data_dir', default='data/mnist',
                    help="Directory containing the dataset")
parser.add_argument('--sprite_filename', default='experiments/malware_sprite.png',
                    help="Sprite image for the projector")

os.environ['CUDA_VISIBLE_DEVICES'] = '0'


def get_train_emb_label():
    tf.reset_default_graph()
    tf.logging.set_verbosity(tf.logging.INFO)

    # Load the parameters from json file
    args = parser.parse_args()
    json_path = os.path.join(args.model_dir, 'params.json')
    assert os.path.isfile(json_path), "No json configuration file found at {}".format(json_path)
    params = Params(json_path)

    LOG_DIR = os.path.join(args.model_dir, "eval")
    path_for_mnist_sprites = os.path.join(LOG_DIR, 'malware_sprite.png')
    path_for_mnist_metadata = os.path.join(LOG_DIR, 'malware_metadata.tsv')

    # Define the model
    tf.logging.info("Creating the model...")
    config = tf.estimator.RunConfig(tf_random_seed=230,
                                    model_dir=args.model_dir,
                                    save_summary_steps=params.save_summary_steps)
    estimator = tf.estimator.Estimator(model_fn, params=params, config=config)

    # Compute embeddings on the test set
    tf.logging.info("Predicting")
    predictions = estimator.predict(lambda: train_input_fn_detect(args.data_dir, params))

    # TODO (@omoindrot): remove the hard-coded 10000
    embeddings = np.zeros((params.train_size, params.embedding_size))
    for i, p in enumerate(predictions):
        print("train",i)
        logging.info(f'train_sample:{i}')
        embeddings[i] = p['embeddings']
        # if(i==0):
        #     print(p['embeddings'])
        #     print(embeddings[i]) ： 第 i 个测试样本的预测向量，64维

    with tf.Session() as sess:
        # TODO (@omoindrot): remove the hard-coded 10000
        # Obtain the test labels
        dataset = mnist_dataset.train(args.data_dir)
        dataset = dataset.map(lambda img, lab: lab)
        dataset = dataset.batch(params.train_size)
        labels_tensor = dataset.make_one_shot_iterator().get_next()
        labels = sess.run(labels_tensor)

    return embeddings, labels



def get_test_emb_label():
    tf.reset_default_graph()
    tf.logging.set_verbosity(tf.logging.INFO)

    # Load the parameters from json file
    args = parser.parse_args()
    json_path = os.path.join(args.model_dir, 'params.json')
    assert os.path.isfile(json_path), "No json configuration file found at {}".format(json_path)
    params = Params(json_path)

    LOG_DIR = os.path.join(args.model_dir, "eval")
    path_for_mnist_sprites = os.path.join(LOG_DIR, 'malware_sprite.png')
    path_for_mnist_metadata = os.path.join(LOG_DIR, 'malware_metadata.tsv')

    # Define the model
    tf.logging.info("Creating the model...")
    config = tf.estimator.RunConfig(tf_random_seed=230,
                                    model_dir=args.model_dir,
                                    save_summary_steps=params.save_summary_steps)
    estimator = tf.estimator.Estimator(model_fn, params=params, config=config)


    # Compute embeddings on the test set
    tf.logging.info("Predicting")
    predictions = estimator.predict(lambda: test_input_fn(args.data_dir, params))

    # TODO (@omoindrot): remove the hard-coded 10000
    embeddings = np.zeros((params.eval_size, params.embedding_size))
    for i, p in enumerate(predictions):
        print("test:",i)
        embeddings[i] = p['embeddings']
        # if(i==0):
        #     print(p['embeddings'])
        #     print(embeddings[i]) ： 第 i 个测试样本的预测向量，64维

    with tf.Session() as sess:
        # TODO (@omoindrot): remove the hard-coded 10000
        # Obtain the test labels
        dataset = mnist_dataset.test(args.data_dir)
        dataset = dataset.map(lambda img, lab: lab)
        dataset = dataset.batch(params.eval_size)
        labels_tensor = dataset.make_one_shot_iterator().get_next()
        labels = sess.run(labels_tensor)

    return embeddings, labels



def detect_drift_samples_myself(train_emb, y_train, test_emb, y_test,
                           margin,
                           mad_threshold,
                           all_detect_path, simple_detect_path, training_info_for_detect_path
                           ):
    if os.path.exists(all_detect_path) and os.path.exists(simple_detect_path):
        logging.info('Detection result files exist, no need to redo the detection')
    else:
        '''get latent data for the entire training and testing set'''
        # train_emb , test_emb 已经存在
        z_train = train_emb
        z_test = test_emb


        '''get latent data for each family in the training set'''
        N, N_family, z_family = get_latent_data_for_each_family(z_train, y_train)

        '''get centroid for each family in the latent space'''
        centroids = [np.mean(z_family[i], axis=0) for i in range(N)]
        # centroids = [np.median(z_family[i], axis=0) for i in range(N)]
        logging.debug(f'centroids: {centroids}')

        '''get distance between each training sample and their family's centroid in the latent space '''
        dis_family = get_latent_distance_between_sample_and_centroid(z_family, centroids,
                                                                     margin,
                                                                     N, N_family)

        '''get the MAD for each family'''
        mad_family = get_MAD_for_each_family(dis_family, N, N_family)

        np.savez_compressed(training_info_for_detect_path,
                            z_train=z_train,
                            z_family=z_family,
                            centroids=centroids,
                            dis_family=dis_family,
                            mad_family=mad_family)

        '''detect drifting in the testing set'''
        with open(all_detect_path, 'w') as f1:
            f1.write('sample_idx,is_drift,closest_family,real_label,min_distance,min_anomaly_score\n')
            with open(simple_detect_path, 'w') as f2:
                f2.write('sample_idx,closest_family,real_label,min_distance,min_anomaly_score\n')

                for k in range(len(test_emb)):
                    z_k = test_emb[k]
                    '''get distance between each testing sample and each centroid'''
                    dis_k = [np.linalg.norm(z_k - centroids[i]) for i in range(N)]
                    anomaly_k = [np.abs(dis_k[i] - np.median(dis_family[i])) / mad_family[i] for i in range(N)]
                    logging.debug(f'sample-{k} - dis_k: {dis_k}')
                    logging.debug(f'sample-{k} - anomaly_k: {anomaly_k}')

                    closest_family = np.argmin(dis_k)
                    min_dis = np.min(dis_k)
                    min_anomaly_score = np.min(anomaly_k)

                    if min_anomaly_score > mad_threshold:
                        logging.debug(f'testing sample {k} is drifting')
                        f1.write(f'{k},Y,{closest_family},{y_test[k]},{min_dis},{min_anomaly_score}\n')
                        f2.write(f'{k},{closest_family},{y_test[k]},{min_dis},{min_anomaly_score}\n')
                    else:
                        f1.write(f'{k},N,{closest_family},{y_test[k]},{min_dis},{min_anomaly_score}\n')



def get_latent_data_for_each_family(z_train, y_train):
    N = len(np.unique(y_train))
    N_family = [len(np.where(y_train == family)[0]) for family in range(N)]
    z_family = []
    for family in range(N):
        z_tmp = z_train[np.where(y_train == family)[0]]
        z_family.append(z_tmp)

    z_len = [len(z_family[i]) for i in range(N)]
    logging.debug(f'z_family length: {z_len}')

    return N, N_family, z_family



def get_latent_distance_between_sample_and_centroid(z_family, centroids, margin, N, N_family):
    dis_family = []  # two-dimension list

    for i in range(N): # i: family index
        dis = [np.linalg.norm(z_family[i][j] - centroids[i]) for j in range(N_family[i])]
        dis_family.append(dis)

    dis_len = [len(dis_family[i]) for i in range(N)]
    logging.debug(f'dis_family length: {dis_len}')

    return dis_family


def get_MAD_for_each_family(dis_family, N, N_family):
    mad_family = []
    for i in range(N):
        median = np.median(dis_family[i])
        logging.debug(f'family {i} median: {median}')
        diff_list = [np.abs(dis_family[i][j] - median) for j in range(N_family[i])]
        mad = 1.4826 * np.median(diff_list)  # 1.4826: assuming the underlying distribution is Gaussian
        mad_family.append(mad)
    logging.debug(f'mad_family: {mad_family}')

    return mad_family




# evaluate的相关函数

def read_combined_report_line(line):
    sample_idx, is_drift, closest, real, min_dis, min_score = line.strip().split(',')
    real = int(real)
    closest = int(closest)
    min_dis = float(min_dis)
    min_score = float(min_score)
    return sample_idx, is_drift, closest, real, min_dis, min_score


def get_best_result(precision_list, recall_list):
    best_inspection_cnt = 0
    best_precision = 0
    best_recall = 0
    best_f1 = 0
    for i in range(len(precision_list)):
        try:
            f1 = 2 * precision_list[i] * recall_list[i] / (precision_list[i] + recall_list[i])
            if f1 > best_f1:
                best_f1 = f1
                best_inspection_cnt = i + 1
                best_precision = precision_list[i]
                best_recall = recall_list[i]
        except:
            logging.debug(f'list-{i}\n {traceback.format_exc()}')
            continue
    return best_inspection_cnt, best_precision, best_recall, best_f1


def plot_inspection_effort_pr_value_by_dist(sorted_samples, newfamily, total_new_family, fig_path, pr_result_path):
    TP, FP = 0, 0
    precision_list, recall_list = [], []
    inspection_cnt_list = range(1, len(sorted_samples) + 1)

    with open(pr_result_path, 'w') as f:
        f.write('sample_idx,real,closest,TP,FP,precision,recall\n')
        for sample_idx, values in sorted_samples.items():
            if values[0] == newfamily:
                TP += 1
            else:
                FP += 1

            precision = TP / (TP + FP)
            recall = TP / total_new_family
            precision_list.append(precision)
            recall_list.append(recall)
            f.write(f'{sample_idx},{values[0]},{values[1]},{TP},{FP},{precision},{recall}\n')

        best_inspection_cnt, best_precision, best_recall, best_f1 = get_best_result(precision_list, recall_list)
        best_inspection_percent = best_inspection_cnt / len(precision_list)
        f.write(f'\n\nTotal: {len(sorted_samples)}\n')
        f.write(f'best inspection count: {best_inspection_cnt}, percent: {best_inspection_percent}\n')
        f.write(f'best performance -- precision: {best_precision * 100:.2f}%, recall: {best_recall * 100:.2f}%\
                f1: {best_f1 * 100:.2f}%\n')

    annotation_text = f'inspect {best_inspection_cnt} samples\nP:{best_precision * 100:.2f}%, R:{best_recall * 100:.2f}%\nF1:{best_f1*100:.2f}%'

    fig, ax = plt.subplots()
    ax.plot(inspection_cnt_list, precision_list, label='precision', color='g')
    ax.plot(inspection_cnt_list, recall_list, label='recall', color='b')
    ax.annotate(annotation_text, (best_inspection_cnt, best_precision), fontsize=6, color='red')
    ax.set_title('Precision Recall value as the change of inspection efforts', fontsize=12)
    ax.set_xticks(np.around(np.linspace(0, len(inspection_cnt_list), 10), decimals=0))
    ax.set_xlabel('Inspection Effort (# of Samples)', fontsize=16)
    ax.set_ylabel('Rate', fontsize=16)
    ax.legend(loc='best')
    fig.savefig(fig_path, dpi=200)
    plt.clf()


def evaluate_newfamily_as_drift_by_distance_myself(newfamily, helper_file, mad_threshold,
                                            save_ordered_dis_path, dist_effort_pr_value_fig_path,
                                            dist_one_by_one_check_result_path):       # newfamily=7

    total_new_family = 0
    sample_result_dict = {}
    y_closest = []
    y_real = []

    with open(helper_file, 'r') as f:
        next(f)
        for idx, line in enumerate(f):
            sample_idx, is_drift, closest, real, min_dis, min_score = read_combined_report_line(line)
            y_closest.append(closest)
            y_real.append(real)

            if real == newfamily:
                total_new_family += 1
            if min_score > mad_threshold:
                sample_result_dict[sample_idx] = [real, closest, min_dis, min_score]



    ordered_sample_result_dict = OrderedDict(sorted(sample_result_dict.items(),
                                            key=lambda x: x[1][2],  # 这里从x[1][3]改成x[1][2],原来的3对应于min_dis
                                            reverse=True))
    with open(save_ordered_dis_path, 'w') as f:
        f.write('sample_idx,real_label,min_dis\n')
        for k, v in ordered_sample_result_dict.items():
            f.write(f'{k},{v[0]},{v[2]}\n')

    plot_inspection_effort_pr_value_by_dist(ordered_sample_result_dict, newfamily, total_new_family,
                                            dist_effort_pr_value_fig_path, dist_one_by_one_check_result_path)