"""Define the model."""

#import tensorflow as tf
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

from triplet_loss import batch_all_triplet_loss
from triplet_loss import batch_hard_triplet_loss

import numpy as np
import os


os.environ['CUDA_VISIBLE_DEVICES'] = '0'


def build_model(is_training, images, params):
    """Compute outputs of the model (embeddings for triplet loss).
    Args:
        is_training: (bool) whether we are training or not
        images: (dict) contains the inputs of the graph (features)
                this can be `tf.placeholder` or outputs of `tf.data`
        params: (Params) hyperparameters
    Returns:
        output: (tf.Tensor) output of the model
    """
    out = images

    # out = tf.concat([out, out, out], 3)
    #
    # print(out.shape)

    # Define the number of channels of each convolution,这里的channels应该是卷积核的数量
    # For each block, we do: 3x3 conv -> batch norm -> relu -> 2x2 maxpool

    """Compute outputs of the model (embeddings for triplet loss).
    模型一：五个卷积block, we do: 3x3 conv -> batch norm -> relu -> 2x2 maxpool
           一个卷积层
    """
    num_channels = params.num_channels
    bn_momentum = params.bn_momentum
    channels = [num_channels, num_channels * 2, num_channels * 2, num_channels * 2, num_channels * 2]
    for i, c in enumerate(channels):
        #print(i,"   ",c)
        with tf.variable_scope('block_{}'.format(i+1)):
            out = tf.layers.conv2d(out, c, 3, padding='same')
            #print(out.shape)
            if params.use_batch_norm:
                out = tf.layers.batch_normalization(out, momentum=bn_momentum, training=is_training)
            out = tf.nn.relu(out)
            #print(out.shape)
            out = tf.layers.max_pooling2d(out, 2, 2)
            #print(out.shape)

    #print(out.shape[1:])
    assert out.shape[1:] == [7, 7, num_channels * 2]

    out = tf.reshape(out, [-1, 7 * 7 * num_channels * 2])
    with tf.variable_scope('fc_1'):
        out = tf.layers.dense(out, params.embedding_size)

    # """Compute outputs of the model (embeddings for triplet loss).
    # 模型二：VGG-16
    # """
    # def conv_op(input_op, name, kh, kw, n_out, dh, dw, p):
    #     n_in = input_op.get_shape()[-1].value
    #
    #     with tf.name_scope(name) as scope:
    #         kernel = tf.get_variable(scope + 'w',
    #                                  shape=[kh, kw, n_in, n_out], dtype=tf.float32,
    #                                  initializer=tf.keras.initializers.glorot_normal(seed=1))
    #         conv = tf.nn.conv2d(input_op, kernel, strides=[1, dh, dw, 1], padding='SAME')
    #         biases = tf.Variable(tf.constant(0.0, shape=[n_out], dtype=tf.float32), trainable=True, name='biases')
    #         z = tf.nn.bias_add(conv, biases)
    #         activation = tf.nn.relu(z, name=scope)
    #         p += [kernel, biases]
    #
    #     return activation
    #
    # # 全连接层
    # def fc_op(input_op, name, n_out, p):
    #     n_in = input_op.get_shape()[-1].value
    #
    #     with tf.name_scope(name) as scope:
    #         kernel = tf.get_variable(scope + 'w',
    #                                  shape=[n_in, n_out], dtype=tf.float32,
    #                                  initializer=tf.keras.initializers.glorot_normal(seed=1))
    #         biases = tf.Variable(tf.constant(0.1, shape=[n_out], dtype=tf.float32), trainable=True, name='biases')
    #         activation = tf.nn.relu_layer(input_op, kernel, biases, name=scope)
    #         p += [kernel, biases]
    #
    #     return activation
    #
    # # 最大池化层
    # def mpool_op(input_op, name, kh, kw, dh, dw):
    #     return tf.nn.max_pool(input_op,
    #                           ksize=[1, kh, kw, 1],
    #                           strides=[1, dh, dw, 1],
    #                           padding='SAME',
    #                           name=name)
    #
    # # 网络结构
    # def inference_op(input_op):
    #     parameters = []
    #
    #     # 第一段卷积网络
    #     conv1_1 = conv_op(input_op, name='conv1_1', kh=3, kw=3, n_out=64, dh=1, dw=1, p=parameters)
    #     conv1_2 = conv_op(conv1_1, name='conv1_2', kh=3, kw=3, n_out=64, dh=1, dw=1, p=parameters)
    #     pool1 = mpool_op(conv1_2, name='pool1', kh=2, kw=2, dw=2, dh=2)
    #
    #     # 第二段卷积网络
    #     conv2_1 = conv_op(pool1, name='conv2_1', kh=3, kw=3, n_out=128, dh=1, dw=1, p=parameters)
    #     conv2_2 = conv_op(conv2_1, name='conv2_2', kh=3, kw=3, n_out=128, dh=1, dw=1, p=parameters)
    #     pool2 = mpool_op(conv2_2, name='pool2', kh=2, kw=2, dh=2, dw=2)
    #
    #     # 第三段卷积网络
    #     conv3_1 = conv_op(pool2, name='conv3_1', kh=3, kw=3, n_out=256, dh=1, dw=1, p=parameters)
    #     conv3_2 = conv_op(conv3_1, name='conv3_2', kh=3, kw=3, n_out=256, dh=1, dw=1, p=parameters)
    #     conv3_3 = conv_op(conv3_2, name='conv3_3', kh=3, kw=3, n_out=256, dh=1, dw=1, p=parameters)
    #     pool3 = mpool_op(conv3_3, name='pool3', kh=2, kw=2, dh=2, dw=2)
    #
    #     # 第四段卷积网络
    #     conv4_1 = conv_op(pool3, name='conv4_1', kh=3, kw=3, n_out=512, dh=1, dw=1, p=parameters)
    #     conv4_2 = conv_op(conv4_1, name='conv4_2', kh=3, kw=3, n_out=512, dh=1, dw=1, p=parameters)
    #     conv4_3 = conv_op(conv4_2, name='conv4_3', kh=3, kw=3, n_out=512, dh=1, dw=1, p=parameters)
    #     pool4 = mpool_op(conv4_3, name='pool4', kh=2, kw=2, dh=2, dw=2)
    #
    #     # 第五段卷积网络
    #     conv5_1 = conv_op(pool4, name='conv5_1', kh=3, kw=3, n_out=512, dh=1, dw=1, p=parameters)
    #     conv5_2 = conv_op(conv5_1, name='conv5_1', kh=3, kw=3, n_out=512, dh=1, dw=1, p=parameters)
    #     conv5_3 = conv_op(conv5_2, name='conv5_3', kh=3, kw=3, n_out=512, dh=1, dw=1, p=parameters)
    #     pool5 = mpool_op(conv5_3, name='conv5_3', kh=2, kw=2, dh=2, dw=2)
    #
    #     # 扁平化
    #     shp = pool5.get_shape()
    #     flattened_shape = shp[1].value * shp[2].value * shp[3].value
    #     resh1 = tf.reshape(pool5, [-1, flattened_shape], name='resh1')
    #
    #     # 全连接层
    #     fc6 = fc_op(resh1, name='fc6', n_out=4096, p=parameters)
    #     fc8 = fc_op(fc6, name='fc8', n_out=params.embedding_size, p=parameters)  # 64维向量
    #
    #
    #     return fc8
    #
    # out = inference_op(out)
    # print("out.shape:", out.shape)


    return out


def model_fn(features, labels, mode, params):
    """Model function for tf.estimator
    Args:
        features: input batch of images
        labels: labels of the images
        mode: can be one of tf.estimator.ModeKeys.{TRAIN, EVAL, PREDICT}
        params: contains hyperparameters of the model (ex: `params.learning_rate`)
    Returns:
        model_spec: tf.estimator.EstimatorSpec object
    """
    is_training = (mode == tf.estimator.ModeKeys.TRAIN)

    images = features
    images = tf.reshape(images, [-1, params.image_size, params.image_size, 1])
    assert images.shape[1:] == [params.image_size, params.image_size, 1], "{}".format(images.shape)

    # -----------------------------------------------------------
    # MODEL: define the layers of the model
    with tf.variable_scope('model'):
        # Compute the embeddings with the model
        embeddings = build_model(is_training, images, params)
    embedding_mean_norm = tf.reduce_mean(tf.norm(embeddings, axis=1))
    tf.summary.scalar("embedding_mean_norm", embedding_mean_norm)

    if mode == tf.estimator.ModeKeys.PREDICT:
        predictions = {'embeddings': embeddings}
        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)

    labels = tf.cast(labels, tf.int64)

    # Define triplet loss
    if params.triplet_strategy == "batch_all":
        loss, fraction = batch_all_triplet_loss(labels, embeddings, margin=params.margin,
                                                squared=params.squared)
    elif params.triplet_strategy == "batch_hard":
        loss = batch_hard_triplet_loss(labels, embeddings, margin=params.margin,
                                       squared=params.squared)
    else:
        raise ValueError("Triplet strategy not recognized: {}".format(params.triplet_strategy))

    # -----------------------------------------------------------
    # METRICS AND SUMMARIES
    # Metrics for evaluation using tf.metrics (average over whole dataset)
    # TODO: some other metrics like rank-1 accuracy?
    with tf.variable_scope("metrics"):
        eval_metric_ops = {"embedding_mean_norm": tf.metrics.mean(embedding_mean_norm)}

        if params.triplet_strategy == "batch_all":
            eval_metric_ops['fraction_positive_triplets'] = tf.metrics.mean(fraction)

    if mode == tf.estimator.ModeKeys.EVAL:
        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metric_ops)


    # Summaries for training
    tf.summary.scalar('loss', loss)
    if params.triplet_strategy == "batch_all":
        tf.summary.scalar('fraction_positive_triplets', fraction)

    tf.summary.image('train_image', images, max_outputs=1)

    # Define training step that minimizes the loss with the Adam optimizer
    optimizer = tf.train.AdamOptimizer(params.learning_rate)
    global_step = tf.train.get_global_step()
    if params.use_batch_norm:
        # Add a dependency to update the moving mean and variance for batch normalization
        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):
            train_op = optimizer.minimize(loss, global_step=global_step)
    else:
        train_op = optimizer.minimize(loss, global_step=global_step)

    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)