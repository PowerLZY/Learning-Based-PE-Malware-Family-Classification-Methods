# https://github.com/tensorflow/tensorflow/issues/30321 解决了multi_GPU_train 只使用一个GPU的问题，使batchsize=64 能在8个GPU上跑起来
# 单个GPU只能batchsize=32,4个GPU可以训练起来 batchsize=64,8个GPU也带不起来128的batchsize

import os
import os.path
import glob
import time
import keras
import csv
import numpy as np
import pandas as pd


import tensorflow as tf
from tensorflow import keras
#from tensorflow.keras.utils import multi_gpu_model




from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder
#from keras.utils import np_utils, plot_model, multi_gpu_model
from keras.utils import np_utils, plot_model


np.random.seed(1)

from malconv import Malconv
from preprocess import preprocess
import utils

from keras.models import load_model, Sequential
from keras.optimizers import Adam, SGD
from keras.callbacks import ModelCheckpoint, EarlyStopping

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import argparse


#import tensorflow.compat.v1 as tf
#from tensorflow.compat.v1.keras.backend import set_session



#def limit_gpu_memory(per):
#    config = tf.ConfigProto()
#    config.gpu_options.per_process_gpu_memory_fraction = per
#    config.gpu_options.allow_growth = True
#    set_session(tf.Session(config=config))



os.environ['CUDA_VISIBLE_DEVICES'] = '5'


imagedir = "/home/mayixuan/malconv/BIG/BIG-2015"
epoch = 30
save_parameters = 'malconv-model-size.h5'
Experiment_results = ""


cur_dir = os.getcwd()
os.chdir(imagedir)  # the parent folder with sub-folders

# Get number of samples per family
list_fams = sorted(os.listdir(os.getcwd()), key=str.lower)  # vector of strings with family names
no_imgs = []  # No. of samples per family
for i in range(len(list_fams)):
    os.chdir(list_fams[i])
    len1 = len(glob.glob('*.bytes'))  # assuming the images are stored as 'png' 样本数向下取整
    no_imgs.append(len1)
    os.chdir('..')
num_samples = np.sum(no_imgs)  # total number of all samples

# Compute the labels
y = np.zeros(num_samples)
pos = 0
label = 0
for i in no_imgs:
    print ("Label:%2d\tFamily: %15s\tNumber of images: %d" % (label, list_fams[label], i))
    for j in range(i):
        y[pos] = label
        pos += 1
    label += 1
num_classes = label


# Compute the features
list_paths = []  # List of image paths
print("loading and processing bytes data...")
start_3 = time.time()

list_name = []
for i in range(len(list_fams)):
    for img_file in glob.glob(list_fams[i]+'/*.bytes'):
        list_paths.append(os.path.join(os.getcwd(), img_file))
        list_name.append(img_file)

data = preprocess(list_name, 2000000)  # 200 0000：表示文件都填充到2MB，或截断为2MB，这里返回data和len(doc)，后面只用data[0]
X = data[0]


print("Bytes processed: %d" % (len(X)))
end_3 = time.time()

os.chdir(cur_dir)


# Encoding classes (y) into integers (y_encoded) and then generating one-hot-encoding (Y)
encoder = LabelEncoder()
encoder.fit(y)
y_encoded = encoder.transform(y)
Y = np_utils.to_categorical(y_encoded)



# Create stratified 5-fold subsets
kfold = 10
skf = StratifiedKFold(kfold, shuffle=True, random_state=1)
skfind = [None] * kfold  # skfind[i][0] -> train indices, skfind[i][1] -> test indices   分层抽样
cnt = 0
for index in skf.split(X, y):
    skfind[cnt] = index
    cnt += 1


# Training top_model and saving min training loss weights
num_epochs = epoch
history = []
checkpointer = ModelCheckpoint(filepath=save_parameters,
                               monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=True, mode='max')
for i in range(kfold):
    train_indices = skfind[i][0]
    test_indices = skfind[i][1]
    X_train = X[train_indices]
    Y_train = Y[train_indices]
    X_test = X[test_indices]
    Y_test = Y[test_indices]
    y_test = y[test_indices]


    #str = tf.distribute.MirroredStrategy(devices=["/gpu:0", "/gpu:1", "/gpu:2", "/gpu:3", "/gpu:4", "/gpu:5", "/gpu:6", "/gpu:7"])
    #with str.scope():
    #    model = Malconv(max_len=200000, win_size=500, vocab_size=256)
    #    model = multi_gpu_model(model, gpus=8)

    # 并行处理整个数据集的不同部分数据
    #with tf.device("/cpu:0"):
    #    model = Malconv(max_len=200000, win_size=500, vocab_size=256)
    #parallel_model = multi_gpu_model(model, gpus=4)

    #strategy = tf.distribute.MirroredStrategy(devices=["/gpu:0", "/gpu:1", "/gpu:2", "/gpu:3"])
    #with strategy.scope():
    #    parallel_model = Malconv(max_len=2000000, win_size=500, vocab_size=256)
    #    parallel_model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001, momentum=0.9), metrics=['accuracy'])


    parallel_model = Malconv(max_len=2000000, win_size=500, vocab_size=256)
    parallel_model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001, momentum=0.9), metrics=['accuracy'])

    start = time.time()
    # 无验证集，batchsize为32是因为显存不够用
    h = parallel_model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=num_epochs,
                  batch_size=32, verbose=2, callbacks=[checkpointer])

    end = time.time()
    history.append(h)

    start_2 = time.time()
    y_prob = parallel_model.predict(X_test, verbose=0)
    y_pred = np.argmax(y_prob, axis=1)
    end_2 = time.time()

    # 时间开销
    print()
    print()
    print('-----------------------------------------------------------------------')
    print('第 %d 次交叉验证' % (i+1))
    print('bytes file 处理size用时：%.4f s ' % (end_3 - start_3))
    print("训练一个fold用时: %.4f s" % (end - start))
    print("预测 %d 个样本用时 : %.4f s" % (len(X_test), end_2 - start_2))

    # 一个fold的分类性能
    print(classification_report(y_test, y_pred, digits=4))