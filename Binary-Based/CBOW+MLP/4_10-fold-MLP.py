# 模块导入
import numpy as np
import pandas as pd
import csv
import os
import os.path
import time
import glob
import sys
import argparse
import random
import tensorflow as tf
import zipfile
from six.moves import range
from six.moves.urllib.request import urlretrieve
from sklearn.manifold import TSNE
from sklearn.preprocessing import MinMaxScaler
from tensorboard.plugins import projector

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.cm as colormap
from matplotlib import pylab


# 利用Keras搭建CNN模型
np.random.seed(1)                        
import keras
from keras.utils import np_utils, plot_model
from keras.models import Sequential, load_model, Model
from keras.layers import Dense, Flatten, Dropout
from keras.optimizers import Adam, SGD
from sklearn.model_selection import train_test_split,StratifiedKFold
from keras.callbacks import ModelCheckpoint,EarlyStopping
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder
from keras.applications.imagenet_utils import preprocess_input



os.environ['CUDA_VISIBLE_DEVICES'] = '3'


imagedir = "drift-before-7-family"

Class_Num = 7
epoch_num = 50
save_parameters = 'mlp-model-size.h5'
Experiment_results = ''



cur_dir = os.getcwd()
os.chdir(imagedir)  # the parent folder with sub-folders

# Get number of samples per family
list_fams = sorted(os.listdir(os.getcwd()), key=str.lower)  # vector of strings with family names
no_imgs = []  # No. of samples per family
for i in range(len(list_fams)):
    os.chdir(list_fams[i])
    len1 = len(glob.glob('*.csv'))  # assuming the images are stored as 'png' 样本数向下取整
    no_imgs.append(len1)
    os.chdir('..')
num_samples = np.sum(no_imgs)  # total number of all samples

# Compute the labels
y = np.zeros(num_samples)
pos = 0
label = 0
for i in no_imgs:
    print ("Label:%2d\tFamily: %15s\tNumber of images: %d" % (label, list_fams[label], i))
    for j in range(i):
        y[pos] = label
        pos += 1
    label += 1
num_classes = label


# Compute the features
width, height = (256, 256)
X = np.zeros((num_samples, width, height))
cnt = 0
list_paths = [] # List of image paths
print("Processing files ...")
start_3 = time.time()

All_imgs = []

for i in range(len(list_fams)):
    pro = 1
    for img_file in glob.glob(list_fams[i]+'/*.csv'):

        All_imgs.append(img_file)  # 把所有图片名放入All_imgs数组中
        if pro > no_imgs[i]:
            break
        #print("[%d] Processing image: %s" % (cnt, img_file))
        list_paths.append(os.path.join(os.getcwd(), img_file))
        # ------------------------------------
        # x 数组即一个文件的256*256的矩阵
        f = csv.reader(open(img_file, 'r'))
        x = []
        for c in f:
            x.append(c)
        # -------------------------------------
        for m in range(len(x)):
            for n in range(len(x[m])):
                x[m][n] = float(x[m][n])
        # -------------------------------------
        X[cnt] = x
        cnt += 1
        pro += 1
print("Images processed: %d" %(cnt))
X = X.reshape(X.shape[0], 256*256).astype('float32')   # 将X处理为65536维的输入
end_3 = time.time()

os.chdir(cur_dir)

# Encoding classes (y) into integers (y_encoded) and then generating one-hot-encoding (Y)
encoder = LabelEncoder()
encoder.fit(y)
y_encoded = encoder.transform(y)          # 整数     y: [0,1,2,3,...]
Y = np_utils.to_categorical(y_encoded)    # One-hot  Y: [[1,0,0,0...],[0,1,0,0...]...]



# Create stratified k-fold subsets
kfold = 10  # no. of folds
skf = StratifiedKFold(kfold, random_state=1)
skfind = [None] * kfold  # skfind[i][0] -> train indices, skfind[i][1] -> test indices
cnt = 0
for index in skf.split(X, y):
    skfind[cnt] = index
    cnt += 1


# 构建MLP模型
history = []
conf_mat = np.zeros((len(list_fams), len(list_fams)))  # Initializing the Confusion Matrix
#earlystop = EarlyStopping(monitor='val_accuracy', patience=8, verbose=2, mode='max')
checkpointer = ModelCheckpoint(filepath=save_parameters,
                               monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=True, mode='min')

for i in range(kfold):
    train_indices = skfind[i][0]
    test_indices = skfind[i][1]
    X_train = X[train_indices]
    Y_train = Y[train_indices]
    X_test = X[test_indices]
    Y_test = Y[test_indices]   # one-hot
    y_test = y[test_indices]   # int


    # 生成一个model
    model = Sequential()
    input_shape = 256 * 256

    # Input + FC1
    model.add(Dense(units=512, input_dim=input_shape, kernel_initializer='glorot_uniform', bias_initializer='zeros',
                    activation='relu', name='FC-1'))
    model.add(Dropout(0.2))

    # FC2
    model.add(Dense(units=512, kernel_initializer='glorot_uniform',
                    bias_initializer='zeros', activation='relu', name='FC-2'))
    model.add(Dropout(0.2))

    # output layer
    model.add(Dense(units=Class_Num, activation='softmax', kernel_initializer='glorot_uniform', name='prediction'))

    sgd = SGD(lr=0.001, momentum=0.9)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])

    print(model.summary())

    start = time.time()
    h = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=epoch_num,
                  batch_size=128, verbose=0, callbacks=[checkpointer])
    end = time.time()
    history.append(h)



    # 加载效果最好的模型，在上面做测试
    # model_2 = load_model('model.h5')
    # model_2.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])


    start_2 = time.time()
    y_prob = model.predict(X_test, verbose=0)
    y_pred = np.argmax(y_prob, axis=1)
    end_2 = time.time()


    # 时间开销
    print()
    print()
    print('-----------------------------------------------------------------------')
    print('第%d次交叉验证' % (i + 1))
    print('csv文件处理size用时：%.4f s ' % (end_3 - start_3))
    print("训练一个fold用时: %.4f s" % (end - start))
    print("预测 %d 个样本用时 : %.4f s" % (len(X_test), end_2 - start_2))

    # 一个fold的分类性能
    print(classification_report(y_test, y_pred, digits=4))



