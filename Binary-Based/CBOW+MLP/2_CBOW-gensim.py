import os
import numpy as np
import pandas as pd
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence

import PIL
import matplotlib.pyplot as pyplot
import scipy
import time


def txt_to_matrix(txtname):
#   save_path = 'cbow-saved-model//' + fi
    # 训练模型
    sentences = LineSentence(txtname)

    # size:词向量维度
    # window：上下文词的个数
    # min_count: 可以对字典做截断. 词频少于min_count次数的单词会被丢弃掉, 默认值为5
    # max_vocab_size: 如果所有独立单词个数超过这个，就消除掉其中最不频繁的一个
    # sg: 0对应CBOW算法，1对应skip-gram算法
    # hs: 如果为1则会采用hierarchica·softmax技巧；如果设置为0（defau·t），则negative sampling会被使用，默认为0
    # iter: 迭代次数，默认为5
    model = Word2Vec(sentences, size=256, window=5, min_count=1, workers=1, compute_loss=True, iter=5, sg=0, hs=0)

    # 保存模型
    #model.save(save_path)

    # 加载模型
    # model = Word2Vec.load(save_path)

    # 返回一个和某个词相关的多个词及其相关度
    # items = model.most_similar('FF')
    # for item in items:
    #    print(item[0], item[1])  # 词的相关度，词的内容

    # 获取词汇
    words = model.wv.index2word


    # 获取对应词向量
    vectors = model.wv.vectors


    # 处理训练好的词向量矩阵，按字节升序来排序
    # 判断是否该文件包含的词典数是256（即是否该文件有256字节）
    if len(words) == 256:
        # 将字典里的单词embedding升序排序，得到file matrix
        num = 0
        matrix = []
        for wor in range(256):  # 要找256个words
            for dic in range(len(words)):
                dic_ten = int(words[dic], 16)  # 把单词字符串转换为十进制数
                if (dic_ten == num):
                    matrix.append(vectors[dic])  # 把字典该位置对应的embedding加入到file矩阵中
                    break
            num += 1

        matrix_final = np.array(matrix)


        # 保存升序矩阵（还未归一化为[0.255]的整数）
        # matrix_final = (matrix_final - np.min(matrix_final)) / (np.max(matrix_final) - np.min(matrix_final)) * 255
        # matrix_final = matrix_final.astype(np.int32)
        # scipy.misc.imsave('part-image//' + fi + '.jpg', matrix_final)

        df = pd.DataFrame(matrix_final)
        df.to_csv("/home/mayixuan/bytes-based/2/" + fi + '.csv', index=False, header=0, float_format='%.8f')
    else:
        # 把少于256字节的文件一同处理了，即对没有的字节填充0
        num = 0
        matrix = []
        aaa = np.zeros(256)
        for wor in range(256):  # 要找少于256个words
            instru = 0
            for dic in range(len(words)):
                dic_ten = int(words[dic], 16)  # 把单词字符串转换为十进制数
                if (dic_ten == num):
                    matrix.append(vectors[dic])  # 把字典该位置对应的embedding加入到file矩阵中
                    instru = 1
                    break
            if instru==0:
                matrix.append(aaa)
            num += 1

        matrix_final = np.array(matrix)

        # 保存升序矩阵（还未归一化为[0.255]的整数）
        df = pd.DataFrame(matrix_final)
        df.to_csv("/home/mayixuan/bytes-based/2/" + fi + '.csv', index=False, header=0, float_format='%.8f')
#        f = open('less-than-256byte-files.txt', 'a')  # 以追加方式写入txt文件
#        f.write(txtname)
#        f.write('\n')
#        f.close()
        


# 获取processed-bytes-file中所有.txt文件的名称,不要后缀
def GetTxtName(dir):
    listName = []
    for fileName in os.listdir(dir):
        if os.path.splitext(fileName)[1] == '.txt':
            fileName = os.path.splitext(fileName)[0]
            listName.append(fileName)
    return listName

# list_name 存放所有txt文件的名称，无后缀.txt
list_name = GetTxtName('/home/mayixuan/bytes-based/1')



# 将所有列表中的.txt文件都转换为词向量矩阵并保存在 gensim-log-window5-iter5 中（以.csv的格式）
start = time.time()
for fi in list_name[0:]:
    txt_name = '/home/mayixuan/bytes-based/1/' + fi + '.txt'
    print('%s begain to transform.' % (fi))
    txt_to_matrix(txt_name)
end = time.time()
print('%.5f' % (end-start))


























